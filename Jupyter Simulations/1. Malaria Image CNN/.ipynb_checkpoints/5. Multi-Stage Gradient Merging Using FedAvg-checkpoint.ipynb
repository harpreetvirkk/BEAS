{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://www.kaggle.com/sharp1/malaria-cells-classification-through-keras\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout\n",
    "import pandas as pd\n",
    "import sys\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(filepath, label):\n",
    "    cells = []\n",
    "    labels = []\n",
    "    file = os.listdir(filepath)\n",
    "    for img in file:\n",
    "        try:\n",
    "            image = cv2.imread(filepath + img)\n",
    "            image_from_array = Image.fromarray(image, 'RGB')\n",
    "            size_image = image_from_array.resize((50, 50))\n",
    "            cells.append(np.array(size_image))\n",
    "            labels.append(label)\n",
    "        except AttributeError as e:\n",
    "            print('Skipping file: ', img, e)\n",
    "    print(len(cells), ' Data Points Read!')\n",
    "    return np.array(cells), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n"
     ]
    }
   ],
   "source": [
    "TestParasitizedCells, TestParasitizedLabels = readData('./input/fed/test/Parasitized/', 1)\n",
    "TestUninfectedCells, TestUninfectedLabels  = readData('./input/fed/test/Uninfected/', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genesis_train(file):\n",
    "    \n",
    "    print('Reading Training Data')\n",
    "    \n",
    "    ParasitizedCells, ParasitizedLabels = readData(file + '/Parasitized/', 1)\n",
    "    UninfectedCells, UninfectedLabels  = readData(file + '/Uninfected/', 0)\n",
    "    Cells = np.concatenate((ParasitizedCells, UninfectedCells))\n",
    "    Labels = np.concatenate((ParasitizedLabels, UninfectedLabels))\n",
    "    \n",
    "    print('Reading Testing Data')\n",
    "    \n",
    "    TestCells = np.concatenate((TestParasitizedCells, TestUninfectedCells))\n",
    "    TestLabels = np.concatenate((TestParasitizedLabels, TestUninfectedLabels))\n",
    "    \n",
    "    s = np.arange(Cells.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    Cells = Cells[s]\n",
    "    Labels = Labels[s]\n",
    "    \n",
    "    sTest = np.arange(TestCells.shape[0])\n",
    "    np.random.shuffle(sTest)\n",
    "    TestCells = TestCells[sTest]\n",
    "    TestLabels = TestLabels[sTest]\n",
    "    \n",
    "    num_classes=len(np.unique(Labels))\n",
    "    len_data=len(Cells)\n",
    "    print(len_data, ' Data Points')\n",
    "    \n",
    "    (x_train,x_test)=Cells, TestCells\n",
    "    (y_train,y_test)=Labels, TestLabels\n",
    "    \n",
    "    # Since we're working on image data, we normalize data by divinding 255.\n",
    "    x_train = x_train.astype('float32')/255 \n",
    "    x_test = x_test.astype('float32')/255\n",
    "    train_len=len(x_train)\n",
    "    test_len=len(x_test)\n",
    "    \n",
    "    #Doing One hot encoding as classifier has multiple classes\n",
    "    y_train=keras.utils.to_categorical(y_train,num_classes)\n",
    "    y_test=keras.utils.to_categorical(y_test,num_classes)\n",
    "    \n",
    "    #creating sequential model\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n",
    "#     model.summary()\n",
    "\n",
    "    # compile the model with loss as categorical_crossentropy and using adam optimizer\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    #Fit the model with min batch size as 50[can tune batch size to some factor of 2^power ] \n",
    "    model.fit(x_train, y_train, batch_size=100, epochs=3, verbose=1)\n",
    "    \n",
    "    scores = model.evaluate(x_test, y_test)\n",
    "    print(\"Loss: \", scores[0])        #Loss\n",
    "    print(\"Accuracy: \", scores[1])    #Accuracy\n",
    "\n",
    "    #Saving Model\n",
    "    model.save(\"./weights/global1.h5\")\n",
    "    return len_data, scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_train(file, d, globalId):\n",
    "    \n",
    "    print('Reading Training Data')\n",
    "    \n",
    "    ParasitizedCells, ParasitizedLabels = readData(file + '/Parasitized/', 1)\n",
    "    UninfectedCells, UninfectedLabels  = readData(file + '/Uninfected/', 0)\n",
    "    Cells = np.concatenate((ParasitizedCells, UninfectedCells))\n",
    "    Labels = np.concatenate((ParasitizedLabels, UninfectedLabels))\n",
    "    \n",
    "    print('Reading Testing Data')\n",
    "    \n",
    "    TestCells = np.concatenate((TestParasitizedCells, TestUninfectedCells))\n",
    "    TestLabels = np.concatenate((TestParasitizedLabels, TestUninfectedLabels))\n",
    "    \n",
    "    s = np.arange(Cells.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    Cells = Cells[s]\n",
    "    Labels = Labels[s]\n",
    "    \n",
    "    sTest = np.arange(TestCells.shape[0])\n",
    "    np.random.shuffle(sTest)\n",
    "    TestCells = TestCells[sTest]\n",
    "    TestLabels = TestLabels[sTest]\n",
    "    \n",
    "    num_classes=len(np.unique(Labels))\n",
    "    len_data=len(Cells)\n",
    "    print(len_data, ' Data Points')\n",
    "    \n",
    "    (x_train,x_test)=Cells, TestCells\n",
    "    (y_train,y_test)=Labels, TestLabels\n",
    "    \n",
    "    # Since we're working on image data, we normalize data by divinding 255.\n",
    "    x_train = x_train.astype('float32')/255 \n",
    "    x_test = x_test.astype('float32')/255\n",
    "    train_len=len(x_train)\n",
    "    test_len=len(x_test)\n",
    "    \n",
    "    #Doing One hot encoding as classifier has multiple classes\n",
    "    y_train=keras.utils.to_categorical(y_train,num_classes)\n",
    "    y_test=keras.utils.to_categorical(y_test,num_classes)\n",
    "    \n",
    "    #creating sequential model\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n",
    "    # model.summary()\n",
    "\n",
    "    model.load_weights(\"./weights/global\"+str(globalId)+\".h5\")\n",
    "    \n",
    "    # compile the model with loss as categorical_crossentropy and using adam optimizer\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    #Fit the model with min batch size as 50[can tune batch size to some factor of 2^power ] \n",
    "    model.fit(x_train, y_train, batch_size=100, epochs=3, verbose=1)\n",
    "    \n",
    "    \n",
    "    scores = model.evaluate(x_test, y_test)\n",
    "    print(\"Loss: \", scores[0])        #Loss\n",
    "    print(\"Accuracy: \", scores[1])    #Accuracy\n",
    "\n",
    "    #Saving Model\n",
    "    model.save(\"./weights/\" + str(d) + \".h5\")\n",
    "    return len_data, scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FedAvg ####\n",
    "\n",
    "def getDataLen(trainingDict):\n",
    "    n = 0\n",
    "    for w in trainingDict:\n",
    "        n += trainingDict[w][0]\n",
    "    print('Total number of data points after this round: ', n)\n",
    "    return n\n",
    "\n",
    "def assignWeights(trainingDf, trainingDict):\n",
    "    n = getDataLen(trainingDict)\n",
    "    trainingDf['Weightage'] = trainingDf['DataSize'].apply(lambda x: x/n)\n",
    "    return trainingDf, n\n",
    "    \n",
    "def scale(weight, scaler):\n",
    "    scaledWeights = []\n",
    "    for i in range(len(weight)):\n",
    "        scaledWeights.append(scaler * weight[i])\n",
    "    return scaledWeights\n",
    "\n",
    "def getScaledWeight(d, scaler):\n",
    "    #creating sequential model\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n",
    "    # model.summary()\n",
    "    \n",
    "    fpath = \"./weights/\"+d+\".h5\"\n",
    "    model.load_weights(fpath)\n",
    "    weight = model.get_weights()\n",
    "    return scale(weight, scaler)\n",
    "\n",
    "def avgWeights(scaledWeights):\n",
    "    avg = list()\n",
    "    for weight_list_tuple in zip(*scaledWeights):\n",
    "        layer_mean = tf.math.reduce_sum(weight_list_tuple, axis=0)\n",
    "        avg.append(layer_mean)\n",
    "    return avg\n",
    "\n",
    "def FedAvg(trainingDict):\n",
    "    trainingDf = pd.DataFrame.from_dict(trainingDict, orient='index', columns=['DataSize', 'Accuracy']) \n",
    "    models = []\n",
    "    for i in trainingDict.keys():\n",
    "        models.append(i)\n",
    "    scaledWeights = []\n",
    "    trainingDf, dataLen = assignWeights(trainingDf, trainingDict)\n",
    "    for m in models:\n",
    "        scaledWeights.append(getScaledWeight(m, trainingDf.loc[m]['Weightage']))\n",
    "    fedAvgWeight = avgWeights(scaledWeights)\n",
    "    return fedAvgWeight, dataLen\n",
    "\n",
    "def saveModel(weight, n):\n",
    "    \n",
    "    print('Reading Testing Data')\n",
    "    \n",
    "    TestParasitizedCells, TestParasitizedLabels = readData('./input/fed/test/Parasitized/', 1)\n",
    "    TestUninfectedCells, TestUninfectedLabels  = readData('./input/fed/test/Uninfected/', 0)\n",
    "    TestCells = np.concatenate((TestParasitizedCells, TestUninfectedCells))\n",
    "    TestLabels = np.concatenate((TestParasitizedLabels, TestUninfectedLabels))\n",
    "    \n",
    "    \n",
    "    sTest = np.arange(TestCells.shape[0])\n",
    "    np.random.shuffle(sTest)\n",
    "    TestCells = TestCells[sTest]\n",
    "    TestLabels = TestLabels[sTest]\n",
    "    \n",
    "    num_classes=len(np.unique(TestLabels))\n",
    "    \n",
    "    (x_test) = TestCells\n",
    "    (y_test) = TestLabels\n",
    "    \n",
    "    # Since we're working on image data, we normalize data by divinding 255.\n",
    "    x_test = x_test.astype('float32')/255\n",
    "    test_len=len(x_test)\n",
    "    \n",
    "    #Doing One hot encoding as classifier has multiple classes\n",
    "    y_test=keras.utils.to_categorical(y_test,num_classes)\n",
    "\n",
    "    #creating sequential model\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n",
    "    # model.summary()\n",
    "    \n",
    "    model.set_weights(weight)\n",
    "\n",
    "    # compile the model with loss as categorical_crossentropy and using adam optimizer\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    scores = model.evaluate(x_test, y_test)\n",
    "    print(\"Loss: \", scores[0])        #Loss\n",
    "    print(\"Accuracy: \", scores[1])    #Accuracy\n",
    "\n",
    "    #Saving Model\n",
    "    fpath = \"./weights/global\"+n+\".h5\"\n",
    "    model.save(fpath)\n",
    "    return scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Training Data\n",
      "686  Data Points Read!\n",
      "696  Data Points Read!\n",
      "Reading Testing Data\n",
      "1382  Data Points\n",
      "Epoch 1/3\n",
      "14/14 [==============================] - 2s 115ms/step - loss: 0.7239 - accuracy: 0.4819\n",
      "Epoch 2/3\n",
      "14/14 [==============================] - 1s 97ms/step - loss: 0.6866 - accuracy: 0.5756\n",
      "Epoch 3/3\n",
      "14/14 [==============================] - 1s 95ms/step - loss: 0.6676 - accuracy: 0.6029\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.6398 - accuracy: 0.6487\n",
      "Loss:  0.6398093104362488\n",
      "Accuracy:  0.648741602897644\n"
     ]
    }
   ],
   "source": [
    "globalDict = dict()\n",
    "trainingDict = dict()\n",
    "trainingDict['global1'] = genesis_train('./input/fed/genesis')\n",
    "globalDict['global1'] = trainingDict['global1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Training Data\n",
      "528  Data Points Read!\n",
      "533  Data Points Read!\n",
      "Reading Testing Data\n",
      "1061  Data Points\n",
      "Epoch 1/3\n",
      "11/11 [==============================] - 1s 94ms/step - loss: 0.6578 - accuracy: 0.6273\n",
      "Epoch 2/3\n",
      "11/11 [==============================] - 1s 91ms/step - loss: 0.6259 - accuracy: 0.6426\n",
      "Epoch 3/3\n",
      "11/11 [==============================] - 1s 91ms/step - loss: 0.5909 - accuracy: 0.6912\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.6027 - accuracy: 0.6860\n",
      "Loss:  0.6027119159698486\n",
      "Accuracy:  0.6860402226448059\n",
      "Reading Training Data\n",
      "522  Data Points Read!\n",
      "528  Data Points Read!\n",
      "Reading Testing Data\n",
      "1050  Data Points\n",
      "Epoch 1/3\n",
      "11/11 [==============================] - 1s 93ms/step - loss: 0.6757 - accuracy: 0.6072\n",
      "Epoch 2/3\n",
      "11/11 [==============================] - 1s 97ms/step - loss: 0.6799 - accuracy: 0.5524\n",
      "Epoch 3/3\n",
      "11/11 [==============================] - 1s 98ms/step - loss: 0.6469 - accuracy: 0.6413\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.6363 - accuracy: 0.6180\n",
      "Loss:  0.6362537145614624\n",
      "Accuracy:  0.6179612278938293\n",
      "Reading Training Data\n",
      "692  Data Points Read!\n",
      "655  Data Points Read!\n",
      "Reading Testing Data\n",
      "1347  Data Points\n",
      "Epoch 1/3\n",
      "14/14 [==============================] - 2s 98ms/step - loss: 0.6813 - accuracy: 0.5622\n",
      "Epoch 2/3\n",
      "14/14 [==============================] - 1s 100ms/step - loss: 0.6380 - accuracy: 0.6406\n",
      "Epoch 3/3\n",
      "14/14 [==============================] - 1s 99ms/step - loss: 0.5975 - accuracy: 0.6663\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.6592 - accuracy: 0.6031\n",
      "Loss:  0.6591612696647644\n",
      "Accuracy:  0.6031142473220825\n",
      "Reading Training Data\n",
      "448  Data Points Read!\n",
      "410  Data Points Read!\n",
      "Reading Testing Data\n",
      "858  Data Points\n",
      "Epoch 1/3\n",
      "9/9 [==============================] - 1s 93ms/step - loss: 0.6405 - accuracy: 0.5970\n",
      "Epoch 2/3\n",
      "9/9 [==============================] - 1s 93ms/step - loss: 0.6068 - accuracy: 0.6725\n",
      "Epoch 3/3\n",
      "9/9 [==============================] - 1s 99ms/step - loss: 0.5886 - accuracy: 0.6909\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.5895 - accuracy: 0.6913\n",
      "Loss:  0.5894806385040283\n",
      "Accuracy:  0.6912909746170044\n",
      "Reading Training Data\n",
      "838  Data Points Read!\n",
      "838  Data Points Read!\n",
      "Reading Testing Data\n",
      "1676  Data Points\n",
      "Epoch 1/3\n",
      "17/17 [==============================] - 2s 101ms/step - loss: 0.6506 - accuracy: 0.6187\n",
      "Epoch 2/3\n",
      "17/17 [==============================] - 2s 101ms/step - loss: 0.6104 - accuracy: 0.6747\n",
      "Epoch 3/3\n",
      "17/17 [==============================] - 2s 103ms/step - loss: 0.5822 - accuracy: 0.6960\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.6047 - accuracy: 0.6688\n",
      "Loss:  0.6047382950782776\n",
      "Accuracy:  0.6688393950462341\n"
     ]
    }
   ],
   "source": [
    "trainingDict['d1'] = local_train('./input/fed/d1', 'd1', 1)\n",
    "trainingDict['d2'] = local_train('./input/fed/d2', 'd2', 1)\n",
    "trainingDict['d3'] = local_train('./input/fed/d3', 'd3', 1)\n",
    "trainingDict['d4'] = local_train('./input/fed/d4', 'd4', 1)\n",
    "trainingDict['d5'] = local_train('./input/fed/d5', 'd5', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global1': (1382, 0.648741602897644),\n",
       " 'd1': (1061, 0.6860402226448059),\n",
       " 'd2': (1050, 0.6179612278938293),\n",
       " 'd3': (1347, 0.6031142473220825),\n",
       " 'd4': (858, 0.6912909746170044),\n",
       " 'd5': (1676, 0.6688393950462341)}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points after this round:  7374\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.6263 - accuracy: 0.6411\n",
      "Loss:  0.6185204982757568\n",
      "Accuracy:  0.653630256652832\n"
     ]
    }
   ],
   "source": [
    "NewGlobal, dataLen = FedAvg(trainingDict)\n",
    "trainingDict = {}\n",
    "trainingDict['global2'] = (dataLen, saveModel(NewGlobal, '2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Training Data\n",
      "599  Data Points Read!\n",
      "567  Data Points Read!\n",
      "Reading Testing Data\n",
      "1166  Data Points\n",
      "Epoch 1/3\n",
      "12/12 [==============================] - 2s 97ms/step - loss: 0.6167 - accuracy: 0.6498\n",
      "Epoch 2/3\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 0.6088 - accuracy: 0.6803\n",
      "Epoch 3/3\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 0.5814 - accuracy: 0.6867\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.5709 - accuracy: 0.7246\n",
      "Loss:  0.5709425210952759\n",
      "Accuracy:  0.7246062159538269\n",
      "Reading Training Data\n",
      "418  Data Points Read!\n",
      "395  Data Points Read!\n",
      "Reading Testing Data\n",
      "813  Data Points\n",
      "Epoch 1/3\n",
      "9/9 [==============================] - 1s 89ms/step - loss: 0.6746 - accuracy: 0.5917\n",
      "Epoch 2/3\n",
      "9/9 [==============================] - 1s 90ms/step - loss: 0.6357 - accuracy: 0.6332\n",
      "Epoch 3/3\n",
      "9/9 [==============================] - 1s 93ms/step - loss: 0.6101 - accuracy: 0.7023\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.6087 - accuracy: 0.6632\n",
      "Loss:  0.6087412238121033\n",
      "Accuracy:  0.6632264852523804\n",
      "Reading Training Data\n",
      "716  Data Points Read!\n",
      "729  Data Points Read!\n",
      "Reading Testing Data\n",
      "1445  Data Points\n",
      "Epoch 1/3\n",
      "15/15 [==============================] - 2s 92ms/step - loss: 0.6460 - accuracy: 0.6258\n",
      "Epoch 2/3\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 0.6005 - accuracy: 0.6816\n",
      "Epoch 3/3\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 0.5662 - accuracy: 0.7000\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.5496 - accuracy: 0.7192\n",
      "Loss:  0.5495652556419373\n",
      "Accuracy:  0.7191743850708008\n",
      "Reading Training Data\n",
      "530  Data Points Read!\n",
      "572  Data Points Read!\n",
      "Reading Testing Data\n",
      "1102  Data Points\n",
      "Epoch 1/3\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.6493 - accuracy: 0.6285\n",
      "Epoch 2/3\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.6340 - accuracy: 0.6572\n",
      "Epoch 3/3\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.6049 - accuracy: 0.6864\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.5994 - accuracy: 0.6717\n",
      "Loss:  0.5993973016738892\n",
      "Accuracy:  0.6717363595962524\n",
      "Reading Training Data\n",
      "695  Data Points Read!\n",
      "701  Data Points Read!\n",
      "Reading Testing Data\n",
      "1396  Data Points\n",
      "Epoch 1/3\n",
      "14/14 [==============================] - 2s 106ms/step - loss: 0.6456 - accuracy: 0.6188\n",
      "Epoch 2/3\n",
      "14/14 [==============================] - 2s 105ms/step - loss: 0.5831 - accuracy: 0.7008\n",
      "Epoch 3/3\n",
      "14/14 [==============================] - 2s 109ms/step - loss: 0.5563 - accuracy: 0.7208\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.5939 - accuracy: 0.6710\n",
      "Loss:  0.5938898921012878\n",
      "Accuracy:  0.6710121035575867\n"
     ]
    }
   ],
   "source": [
    "globalDict['global2'] = trainingDict['global2']\n",
    "trainingDict['d6'] = local_train('./input/fed/d6', 'd6', 2)\n",
    "trainingDict['d7'] = local_train('./input/fed/d7', 'd7', 2)\n",
    "trainingDict['d8'] = local_train('./input/fed/d8', 'd8', 2)\n",
    "trainingDict['d9'] = local_train('./input/fed/d9', 'd9', 2)\n",
    "trainingDict['d10'] = local_train('./input/fed/d10', 'd10', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points after this round:  13296\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.5986 - accuracy: 0.6764\n",
      "Loss:  0.6015595197677612\n",
      "Accuracy:  0.6686583161354065\n"
     ]
    }
   ],
   "source": [
    "NewGlobal, dataLen = FedAvg(trainingDict)\n",
    "trainingDict = {}\n",
    "trainingDict['global3'] = (dataLen, saveModel(NewGlobal, '3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Training Data\n",
      "557  Data Points Read!\n",
      "577  Data Points Read!\n",
      "Reading Testing Data\n",
      "1134  Data Points\n",
      "Epoch 1/3\n",
      "12/12 [==============================] - 2s 89ms/step - loss: 0.6427 - accuracy: 0.6392\n",
      "Epoch 2/3\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 0.6028 - accuracy: 0.6713\n",
      "Epoch 3/3\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.5802 - accuracy: 0.6796\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.5819 - accuracy: 0.6815\n",
      "Loss:  0.5818642377853394\n",
      "Accuracy:  0.6815136671066284\n",
      "Reading Training Data\n",
      "827  Data Points Read!\n",
      "796  Data Points Read!\n",
      "Reading Testing Data\n",
      "1623  Data Points\n",
      "Epoch 1/3\n",
      "17/17 [==============================] - 2s 92ms/step - loss: 0.5924 - accuracy: 0.6799\n",
      "Epoch 2/3\n",
      "17/17 [==============================] - 2s 93ms/step - loss: 0.5671 - accuracy: 0.7178\n",
      "Epoch 3/3\n",
      "17/17 [==============================] - 2s 95ms/step - loss: 0.5152 - accuracy: 0.7643\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.5252 - accuracy: 0.7337\n",
      "Loss:  0.5251858234405518\n",
      "Accuracy:  0.7336592674255371\n",
      "Reading Training Data\n",
      "395  Data Points Read!\n",
      "425  Data Points Read!\n",
      "Reading Testing Data\n",
      "820  Data Points\n",
      "Epoch 1/3\n",
      "9/9 [==============================] - 1s 92ms/step - loss: 0.6206 - accuracy: 0.6716\n",
      "Epoch 2/3\n",
      "9/9 [==============================] - 1s 92ms/step - loss: 0.5483 - accuracy: 0.7360\n",
      "Epoch 3/3\n",
      "9/9 [==============================] - 1s 94ms/step - loss: 0.5344 - accuracy: 0.7497\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.6196 - accuracy: 0.6732\n",
      "Loss:  0.6195995807647705\n",
      "Accuracy:  0.673184871673584\n",
      "Reading Training Data\n",
      "513  Data Points Read!\n",
      "528  Data Points Read!\n",
      "Reading Testing Data\n",
      "1041  Data Points\n",
      "Epoch 1/3\n",
      "11/11 [==============================] - 1s 93ms/step - loss: 0.6655 - accuracy: 0.5955\n",
      "Epoch 2/3\n",
      "11/11 [==============================] - 1s 132ms/step - loss: 0.6194 - accuracy: 0.6645\n",
      "Epoch 3/3\n",
      "11/11 [==============================] - 1s 116ms/step - loss: 0.5826 - accuracy: 0.6942\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.5697 - accuracy: 0.7201\n",
      "Loss:  0.5696674585342407\n",
      "Accuracy:  0.7200796604156494\n",
      "Reading Training Data\n",
      "284  Data Points Read!\n",
      "281  Data Points Read!\n",
      "Reading Testing Data\n",
      "565  Data Points\n",
      "Epoch 1/3\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 0.5855 - accuracy: 0.6772\n",
      "Epoch 2/3\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 0.5906 - accuracy: 0.6894\n",
      "Epoch 3/3\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 0.5581 - accuracy: 0.7065\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.5820 - accuracy: 0.6994\n",
      "Loss:  0.5820282101631165\n",
      "Accuracy:  0.6994386911392212\n"
     ]
    }
   ],
   "source": [
    "globalDict['global3'] = trainingDict['global3']\n",
    "trainingDict['d11'] = local_train('./input/fed/d11', 'd11', 3)\n",
    "trainingDict['d12'] = local_train('./input/fed/d12', 'd12', 3)\n",
    "trainingDict['d13'] = local_train('./input/fed/d13', 'd13', 3)\n",
    "trainingDict['d14'] = local_train('./input/fed/d14', 'd14', 3)\n",
    "trainingDict['d15'] = local_train('./input/fed/d15', 'd15', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points after this round:  18479\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.5838 - accuracy: 0.6863\n",
      "Loss:  0.5876637697219849\n",
      "Accuracy:  0.6802462339401245\n"
     ]
    }
   ],
   "source": [
    "NewGlobal, dataLen = FedAvg(trainingDict)\n",
    "trainingDict = {}\n",
    "trainingDict['global4'] = (dataLen, saveModel(NewGlobal, '4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Training Data\n",
      "412  Data Points Read!\n",
      "416  Data Points Read!\n",
      "Reading Testing Data\n",
      "828  Data Points\n",
      "Epoch 1/3\n",
      "9/9 [==============================] - 1s 99ms/step - loss: 0.6072 - accuracy: 0.6974\n",
      "Epoch 2/3\n",
      "9/9 [==============================] - 1s 99ms/step - loss: 0.5829 - accuracy: 0.7050\n",
      "Epoch 3/3\n",
      "9/9 [==============================] - 1s 101ms/step - loss: 0.5677 - accuracy: 0.7368\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.5659 - accuracy: 0.7032\n",
      "Loss:  0.5659196376800537\n",
      "Accuracy:  0.7032409906387329\n",
      "Reading Training Data\n",
      "417  Data Points Read!\n",
      "414  Data Points Read!\n",
      "Reading Testing Data\n",
      "831  Data Points\n",
      "Epoch 1/3\n",
      "9/9 [==============================] - 2s 98ms/step - loss: 0.5919 - accuracy: 0.6786\n",
      "Epoch 2/3\n",
      "9/9 [==============================] - 1s 98ms/step - loss: 0.5734 - accuracy: 0.6997\n",
      "Epoch 3/3\n",
      "9/9 [==============================] - 1s 99ms/step - loss: 0.5227 - accuracy: 0.7342\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.5492 - accuracy: 0.7099\n",
      "Loss:  0.5491872429847717\n",
      "Accuracy:  0.7099402546882629\n",
      "Reading Training Data\n",
      "269  Data Points Read!\n",
      "252  Data Points Read!\n",
      "Reading Testing Data\n",
      "521  Data Points\n",
      "Epoch 1/3\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 0.5969 - accuracy: 0.6895\n",
      "Epoch 2/3\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 0.6019 - accuracy: 0.6891\n",
      "Epoch 3/3\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 0.5509 - accuracy: 0.7559\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.5564 - accuracy: 0.7237\n",
      "Loss:  0.5563501715660095\n",
      "Accuracy:  0.7237008810043335\n",
      "Reading Training Data\n",
      "407  Data Points Read!\n",
      "407  Data Points Read!\n",
      "Reading Testing Data\n",
      "814  Data Points\n",
      "Epoch 1/3\n",
      "9/9 [==============================] - 1s 92ms/step - loss: 0.6052 - accuracy: 0.6624\n",
      "Epoch 2/3\n",
      "9/9 [==============================] - 1s 92ms/step - loss: 0.5995 - accuracy: 0.6857\n",
      "Epoch 3/3\n",
      "9/9 [==============================] - 1s 93ms/step - loss: 0.5957 - accuracy: 0.6810\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.6225 - accuracy: 0.6692\n",
      "Loss:  0.6224715709686279\n",
      "Accuracy:  0.6692014932632446\n",
      "Reading Training Data\n",
      "286  Data Points Read!\n",
      "276  Data Points Read!\n",
      "Reading Testing Data\n",
      "562  Data Points\n",
      "Epoch 1/3\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 0.6179 - accuracy: 0.6713\n",
      "Epoch 2/3\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.5802 - accuracy: 0.6978\n",
      "Epoch 3/3\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.5576 - accuracy: 0.7142\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.5808 - accuracy: 0.6873\n",
      "Loss:  0.5808253884315491\n",
      "Accuracy:  0.687307596206665\n"
     ]
    }
   ],
   "source": [
    "globalDict['global4'] = trainingDict['global4']\n",
    "trainingDict['d16'] = local_train('./input/fed/d16', 'd16', 4)\n",
    "trainingDict['d17'] = local_train('./input/fed/d17', 'd17', 4)\n",
    "trainingDict['d18'] = local_train('./input/fed/d18', 'd18', 4)\n",
    "trainingDict['d19'] = local_train('./input/fed/d19', 'd19', 4)\n",
    "trainingDict['d20'] = local_train('./input/fed/d20', 'd20', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points after this round:  22035\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.5818 - accuracy: 0.6879\n",
      "Loss:  0.5797343850135803\n",
      "Accuracy:  0.68966144323349\n"
     ]
    }
   ],
   "source": [
    "NewGlobal, dataLen = FedAvg(trainingDict)\n",
    "trainingDict = {}\n",
    "trainingDict['global5'] = (dataLen, saveModel(NewGlobal, '5'))\n",
    "globalDict['global5'] = trainingDict['global5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global1': (1382, 0.648741602897644),\n",
       " 'global2': (7374, 0.653630256652832),\n",
       " 'global3': (13296, 0.6686583161354065),\n",
       " 'global4': (18479, 0.6802462339401245),\n",
       " 'global5': (22035, 0.68966144323349)}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
