{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout\n",
    "import pandas as pd\n",
    "import sys\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(filepath, label):\n",
    "    cells = []\n",
    "    labels = []\n",
    "    file = os.listdir(filepath)\n",
    "    for img in file:\n",
    "        try:\n",
    "            image = cv2.imread(filepath + img)\n",
    "            image_from_array = Image.fromarray(image, 'RGB')\n",
    "            size_image = image_from_array.resize((50, 50))\n",
    "            cells.append(np.array(size_image))\n",
    "            labels.append(label)\n",
    "        except AttributeError as e:\n",
    "            print('Skipping file: ', img, e)\n",
    "    print(len(cells), ' Data Points Read!')\n",
    "    return np.array(cells), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genesis_train(file):\n",
    "    \n",
    "    print('Reading Training Data')\n",
    "    \n",
    "    ParasitizedCells, ParasitizedLabels = readData(file + '/Parasitized/', 1)\n",
    "    UninfectedCells, UninfectedLabels  = readData(file + '/Uninfected/', 0)\n",
    "    Cells = np.concatenate((ParasitizedCells, UninfectedCells))\n",
    "    Labels = np.concatenate((ParasitizedLabels, UninfectedLabels))\n",
    "    \n",
    "    print('Reading Testing Data')\n",
    "    \n",
    "    TestParasitizedCells, TestParasitizedLabels = readData('./input/fed/test/Parasitized/', 1)\n",
    "    TestUninfectedCells, TestUninfectedLabels  = readData('./input/fed/test/Uninfected/', 0)\n",
    "    TestCells = np.concatenate((TestParasitizedCells, TestUninfectedCells))\n",
    "    TestLabels = np.concatenate((TestParasitizedLabels, TestUninfectedLabels))\n",
    "    \n",
    "    s = np.arange(Cells.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    Cells = Cells[s]\n",
    "    Labels = Labels[s]\n",
    "    \n",
    "    sTest = np.arange(TestCells.shape[0])\n",
    "    np.random.shuffle(sTest)\n",
    "    TestCells = TestCells[sTest]\n",
    "    TestLabels = TestLabels[sTest]\n",
    "    \n",
    "    num_classes=len(np.unique(Labels))\n",
    "    len_data=len(Cells)\n",
    "    print(len_data, ' Data Points')\n",
    "    \n",
    "    (x_train,x_test)=Cells, TestCells\n",
    "    (y_train,y_test)=Labels, TestLabels\n",
    "    \n",
    "    # Since we're working on image data, we normalize data by divinding 255.\n",
    "    x_train = x_train.astype('float32')/255 \n",
    "    x_test = x_test.astype('float32')/255\n",
    "    train_len=len(x_train)\n",
    "    test_len=len(x_test)\n",
    "    \n",
    "    #Doing One hot encoding as classifier has multiple classes\n",
    "    y_train=keras.utils.to_categorical(y_train,num_classes)\n",
    "    y_test=keras.utils.to_categorical(y_test,num_classes)\n",
    "    \n",
    "    #creating sequential model\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n",
    "#     model.summary()\n",
    "\n",
    "    # compile the model with loss as categorical_crossentropy and using adam optimizer\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    #Fit the model with min batch size as 50[can tune batch size to some factor of 2^power ] \n",
    "    model.fit(x_train, y_train, batch_size=100, epochs=5, verbose=1)\n",
    "    \n",
    "    scores = model.evaluate(x_test, y_test)\n",
    "    print(\"Loss: \", scores[0])        #Loss\n",
    "    print(\"Accuracy: \", scores[1])    #Accuracy\n",
    "\n",
    "    #Saving Model\n",
    "    model.save(\"./weights/global1.h5\")\n",
    "    return len_data, scores[0], scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_train(file, d, globalId):\n",
    "    \n",
    "    print('Reading Training Data')\n",
    "    \n",
    "    ParasitizedCells, ParasitizedLabels = readData(file + '/Parasitized/', 1)\n",
    "    UninfectedCells, UninfectedLabels  = readData(file + '/Uninfected/', 0)\n",
    "    Cells = np.concatenate((ParasitizedCells, UninfectedCells))\n",
    "    Labels = np.concatenate((ParasitizedLabels, UninfectedLabels))\n",
    "    \n",
    "    print('Reading Testing Data')\n",
    "    \n",
    "    TestParasitizedCells, TestParasitizedLabels = readData('./input/fed/test/Parasitized/', 1)\n",
    "    TestUninfectedCells, TestUninfectedLabels  = readData('./input/fed/test/Uninfected/', 0)\n",
    "    TestCells = np.concatenate((TestParasitizedCells, TestUninfectedCells))\n",
    "    TestLabels = np.concatenate((TestParasitizedLabels, TestUninfectedLabels))\n",
    "    \n",
    "    s = np.arange(Cells.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    Cells = Cells[s]\n",
    "    Labels = Labels[s]\n",
    "    \n",
    "    sTest = np.arange(TestCells.shape[0])\n",
    "    np.random.shuffle(sTest)\n",
    "    TestCells = TestCells[sTest]\n",
    "    TestLabels = TestLabels[sTest]\n",
    "    \n",
    "    num_classes=len(np.unique(Labels))\n",
    "    len_data=len(Cells)\n",
    "    print(len_data, ' Data Points')\n",
    "    \n",
    "    (x_train,x_test)=Cells, TestCells\n",
    "    (y_train,y_test)=Labels, TestLabels\n",
    "    \n",
    "    # Since we're working on image data, we normalize data by divinding 255.\n",
    "    x_train = x_train.astype('float32')/255 \n",
    "    x_test = x_test.astype('float32')/255\n",
    "    train_len=len(x_train)\n",
    "    test_len=len(x_test)\n",
    "    \n",
    "    #Doing One hot encoding as classifier has multiple classes\n",
    "    y_train=keras.utils.to_categorical(y_train,num_classes)\n",
    "    y_test=keras.utils.to_categorical(y_test,num_classes)\n",
    "    \n",
    "    #creating sequential model\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n",
    "    # model.summary()\n",
    "\n",
    "    model.load_weights(\"./weights/global\"+str(globalId)+\".h5\")\n",
    "    \n",
    "    # compile the model with loss as categorical_crossentropy and using adam optimizer\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    #Fit the model with min batch size as 50[can tune batch size to some factor of 2^power ] \n",
    "    model.fit(x_train, y_train, batch_size=100, epochs=5, verbose=1)\n",
    "    \n",
    "    \n",
    "    scores = model.evaluate(x_test, y_test)\n",
    "    print(\"Loss: \", scores[0])        #Loss\n",
    "    print(\"Accuracy: \", scores[1])    #Accuracy\n",
    "\n",
    "    #Saving Model\n",
    "    model.save(\"./weights/\" + str(d) + \".h5\")\n",
    "    return len_data, scores[0], scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Zeno ####\n",
    "\n",
    "def getDataLen(trainingDict):\n",
    "    n = 0\n",
    "    for w in trainingDict:\n",
    "        n += trainingDict[w][0]\n",
    "    print('Total number of data points after this round: ', n)\n",
    "    return n\n",
    "\n",
    "def assignWeights(trainingDf, trainingDict):\n",
    "    n = getDataLen(trainingDict)\n",
    "    trainingDf['Weightage'] = trainingDf['DataSize'].apply(lambda x: x/n)\n",
    "    return trainingDf, n\n",
    "    \n",
    "def scale(weight, scaler):\n",
    "    scaledWeights = []\n",
    "    for i in range(len(weight)):\n",
    "        scaledWeights.append(scaler * weight[i])\n",
    "    return scaledWeights\n",
    "\n",
    "def getScaledWeight(d, scaler):\n",
    "    #creating sequential model\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n",
    "    # model.summary()\n",
    "    \n",
    "    fpath = \"./weights/\"+d+\".h5\"\n",
    "    model.load_weights(fpath)\n",
    "    weight = model.get_weights()\n",
    "    return scale(weight, scaler)\n",
    "\n",
    "def avgWeights(scaledWeights):\n",
    "    avg = list()\n",
    "    for weight_list_tuple in zip(*scaledWeights):\n",
    "        layer_mean = tf.math.reduce_sum(weight_list_tuple, axis=0)\n",
    "        avg.append(layer_mean)\n",
    "    return avg\n",
    "\n",
    "def FedAvg(trainingDict):\n",
    "    trainingDf = pd.DataFrame.from_dict(trainingDict, orient='index', columns=['DataSize', 'Loss','Accuracy']) \n",
    "    models = []\n",
    "    for i in trainingDict.keys():\n",
    "        models.append(i)\n",
    "    scaledWeights = []\n",
    "    trainingDf, dataLen = assignWeights(trainingDf, trainingDict)\n",
    "    for m in models:\n",
    "        scaledWeights.append(getScaledWeight(m, trainingDf.loc[m]['Weightage']))\n",
    "    fedAvgWeight = avgWeights(scaledWeights)\n",
    "    return fedAvgWeight, dataLen\n",
    "\n",
    "\n",
    "def ZenoScore(local, g_loss, p):\n",
    "    scored = {}\n",
    "    for l in local.keys():\n",
    "        score = g_loss - local[l][1] - (local[l][0]*p)\n",
    "        scored[l] = score\n",
    "    return scored\n",
    "\n",
    "def zeno(trainingDict, p, b):\n",
    "    \n",
    "    models = list(trainingDict.keys())\n",
    "    \n",
    "    trainingDf = pd.DataFrame.from_dict(trainingDict, orient='index', columns=['DataSize', 'Loss', 'Accuracy']) \n",
    "    \n",
    "    g_loss = 0    \n",
    "    local = {}\n",
    "    \n",
    "    for i in trainingDict.keys():\n",
    "        if 'global' in i:\n",
    "            g_loss = trainingDict[i][1]\n",
    "        else:\n",
    "            local[i] = (trainingDict[i])\n",
    "            \n",
    "    \n",
    "    scores = ZenoScore(local, g_loss, p)\n",
    "    b = int(len(scores)*b)\n",
    "    sortedScores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1])}\n",
    "    \n",
    "    zenoedModels = []\n",
    "    \n",
    "    for i in range(b):\n",
    "        zenoedModels.append((sortedScores.popitem())[0])\n",
    "        \n",
    "    newDict = {}\n",
    "    for i in trainingDict.keys():\n",
    "        if i not in zenoedModels:\n",
    "            newDict[i] = trainingDict[i]\n",
    "            \n",
    "    print('Zeno Selections: ', zenoedModels)\n",
    "        \n",
    "#     NewGlobal, dataLen = FedAvg(trainingDict) !!!!!\n",
    "    NewGlobal, dataLen = FedAvg(zenoedModels)\n",
    "    \n",
    "    return NewGlobal, dataLen\n",
    "\n",
    "def saveModel(weight, n):\n",
    "    \n",
    "    print('Reading Testing Data')\n",
    "    \n",
    "    TestParasitizedCells, TestParasitizedLabels = readData('./input/fed/test/Parasitized/', 1)\n",
    "    TestUninfectedCells, TestUninfectedLabels  = readData('./input/fed/test/Uninfected/', 0)\n",
    "    TestCells = np.concatenate((TestParasitizedCells, TestUninfectedCells))\n",
    "    TestLabels = np.concatenate((TestParasitizedLabels, TestUninfectedLabels))\n",
    "    \n",
    "    \n",
    "    sTest = np.arange(TestCells.shape[0])\n",
    "    np.random.shuffle(sTest)\n",
    "    TestCells = TestCells[sTest]\n",
    "    TestLabels = TestLabels[sTest]\n",
    "    \n",
    "    num_classes=len(np.unique(TestLabels))\n",
    "    \n",
    "    (x_test) = TestCells\n",
    "    (y_test) = TestLabels\n",
    "    \n",
    "    # Since we're working on image data, we normalize data by divinding 255.\n",
    "    x_test = x_test.astype('float32')/255\n",
    "    test_len=len(x_test)\n",
    "    \n",
    "    #Doing One hot encoding as classifier has multiple classes\n",
    "    y_test=keras.utils.to_categorical(y_test,num_classes)\n",
    "\n",
    "    #creating sequential model\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n",
    "    # model.summary()\n",
    "    \n",
    "    model.set_weights(weight)\n",
    "\n",
    "    # compile the model with loss as categorical_crossentropy and using adam optimizer\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    scores = model.evaluate(x_test, y_test)\n",
    "    print(\"Loss: \", scores[0])        #Loss\n",
    "    print(\"Accuracy: \", scores[1])    #Accuracy\n",
    "\n",
    "    #Saving Model\n",
    "    fpath = \"./weights/global\"+n+\".h5\"\n",
    "    model.save(fpath)\n",
    "    return scores[0], scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Training Data\n",
      "686  Data Points Read!\n",
      "696  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1382  Data Points\n",
      "Epoch 1/5\n",
      "14/14 [==============================] - 2s 134ms/step - loss: 0.7411 - accuracy: 0.4906\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 2s 121ms/step - loss: 0.6945 - accuracy: 0.5123\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 2s 117ms/step - loss: 0.6868 - accuracy: 0.5839\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 2s 120ms/step - loss: 0.6703 - accuracy: 0.6085\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 2s 121ms/step - loss: 0.6379 - accuracy: 0.6404\n",
      "173/173 [==============================] - 2s 13ms/step - loss: 0.6063 - accuracy: 0.6701\n",
      "Loss:  0.6062763929367065\n",
      "Accuracy:  0.670106828212738\n"
     ]
    }
   ],
   "source": [
    "globalDict = dict()\n",
    "trainingDict = dict()\n",
    "trainingDict['global1'] = genesis_train('./input/fed/genesis')\n",
    "globalDict['global1'] = trainingDict['global1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Training Data\n",
      "528  Data Points Read!\n",
      "533  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1061  Data Points\n",
      "Epoch 1/5\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.6048 - accuracy: 0.6598\n",
      "Epoch 2/5\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.5813 - accuracy: 0.7022\n",
      "Epoch 3/5\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.5504 - accuracy: 0.7418\n",
      "Epoch 4/5\n",
      "11/11 [==============================] - 1s 126ms/step - loss: 0.5368 - accuracy: 0.7276\n",
      "Epoch 5/5\n",
      "11/11 [==============================] - 2s 142ms/step - loss: 0.4889 - accuracy: 0.7879\n",
      "173/173 [==============================] - 3s 15ms/step - loss: 0.5252 - accuracy: 0.7509\n",
      "Loss:  0.5252317786216736\n",
      "Accuracy:  0.7508600354194641\n",
      "Reading Training Data\n",
      "522  Data Points Read!\n",
      "528  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1050  Data Points\n",
      "Epoch 1/5\n",
      "11/11 [==============================] - 1s 85ms/step - loss: 0.6164 - accuracy: 0.6571\n",
      "Epoch 2/5\n",
      "11/11 [==============================] - 1s 84ms/step - loss: 0.5900 - accuracy: 0.6933\n",
      "Epoch 3/5\n",
      "11/11 [==============================] - 1s 84ms/step - loss: 0.5885 - accuracy: 0.6914\n",
      "Epoch 4/5\n",
      "11/11 [==============================] - 1s 84ms/step - loss: 0.5559 - accuracy: 0.7238\n",
      "Epoch 5/5\n",
      "11/11 [==============================] - 1s 87ms/step - loss: 0.5279 - accuracy: 0.7305\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.5274 - accuracy: 0.7318\n",
      "Loss:  0.5273655652999878\n",
      "Accuracy:  0.7318486571311951\n",
      "Reading Training Data\n",
      "692  Data Points Read!\n",
      "655  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1347  Data Points\n",
      "Epoch 1/5\n",
      "14/14 [==============================] - 1s 96ms/step - loss: 0.6240 - accuracy: 0.6540\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 1s 91ms/step - loss: 0.5903 - accuracy: 0.6934\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 1s 92ms/step - loss: 0.5557 - accuracy: 0.7179\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 1s 90ms/step - loss: 0.5232 - accuracy: 0.7595\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 1s 88ms/step - loss: 0.4856 - accuracy: 0.7854\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.4979 - accuracy: 0.7619\n",
      "Loss:  0.4978816509246826\n",
      "Accuracy:  0.761904776096344\n",
      "Reading Training Data\n",
      "448  Data Points Read!\n",
      "410  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "858  Data Points\n",
      "Epoch 1/5\n",
      "9/9 [==============================] - 1s 79ms/step - loss: 0.6455 - accuracy: 0.6305\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 1s 81ms/step - loss: 0.6043 - accuracy: 0.6900\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 1s 80ms/step - loss: 0.5815 - accuracy: 0.7051\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 0.5637 - accuracy: 0.6970\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 1s 81ms/step - loss: 0.5479 - accuracy: 0.7378\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.5583 - accuracy: 0.7054\n",
      "Loss:  0.5582518577575684\n",
      "Accuracy:  0.7054136991500854\n",
      "Reading Training Data\n",
      "838  Data Points Read!\n",
      "838  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1676  Data Points\n",
      "Epoch 1/5\n",
      "17/17 [==============================] - 2s 93ms/step - loss: 0.6235 - accuracy: 0.6510\n",
      "Epoch 2/5\n",
      "17/17 [==============================] - 2s 92ms/step - loss: 0.5724 - accuracy: 0.7076\n",
      "Epoch 3/5\n",
      "17/17 [==============================] - 2s 92ms/step - loss: 0.5537 - accuracy: 0.7214\n",
      "Epoch 4/5\n",
      "17/17 [==============================] - 2s 93ms/step - loss: 0.5175 - accuracy: 0.7679\n",
      "Epoch 5/5\n",
      "17/17 [==============================] - 2s 92ms/step - loss: 0.4856 - accuracy: 0.7757\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.4765 - accuracy: 0.7985\n",
      "Loss:  0.47650906443595886\n",
      "Accuracy:  0.7984790802001953\n"
     ]
    }
   ],
   "source": [
    "trainingDict['d1'] = local_train('./input/fed/d1', 'd1', 1)\n",
    "trainingDict['d2'] = local_train('./input/fed/d2', 'd2', 1)\n",
    "trainingDict['d3'] = local_train('./input/fed/d3', 'd3', 1)\n",
    "trainingDict['d4'] = local_train('./input/fed/d4', 'd4', 1)\n",
    "trainingDict['d5'] = local_train('./input/fed/d5', 'd5', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global1': (1382, 0.6062763929367065, 0.670106828212738),\n",
       " 'd1': (1061, 0.5252317786216736, 0.7508600354194641),\n",
       " 'd2': (1050, 0.5273655652999878, 0.7318486571311951),\n",
       " 'd3': (1347, 0.4978816509246826, 0.761904776096344),\n",
       " 'd4': (858, 0.5582518577575684, 0.7054136991500854),\n",
       " 'd5': (1676, 0.47650906443595886, 0.7984790802001953)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeno Selections:  ['d4', 'd2', 'd1']\n",
      "Total number of data points after this round:  7374\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.5277 - accuracy: 0.7358\n",
      "Loss:  0.5276505947113037\n",
      "Accuracy:  0.7358319759368896\n"
     ]
    }
   ],
   "source": [
    "NewGlobal, dataLen = zeno(trainingDict, 0.0005, 0.75)\n",
    "trainingDict = {}\n",
    "newModelLoss, newModelAccuracy = saveModel(NewGlobal, '2')\n",
    "trainingDict['global2'] = (dataLen, newModelLoss, newModelAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Training Data\n",
      "599  Data Points Read!\n",
      "567  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1166  Data Points\n",
      "Epoch 1/5\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 0.5461 - accuracy: 0.7247\n",
      "Epoch 2/5\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 0.4951 - accuracy: 0.7710\n",
      "Epoch 3/5\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.4515 - accuracy: 0.8036\n",
      "Epoch 4/5\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.3985 - accuracy: 0.8259\n",
      "Epoch 5/5\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.3461 - accuracy: 0.8542\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.3458 - accuracy: 0.8678\n",
      "Loss:  0.3457932472229004\n",
      "Accuracy:  0.867825448513031\n",
      "Reading Training Data\n",
      "418  Data Points Read!\n",
      "395  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "813  Data Points\n",
      "Epoch 1/5\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 0.5748 - accuracy: 0.7060\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 0.5524 - accuracy: 0.7097\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 0.5007 - accuracy: 0.7798\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 0.4782 - accuracy: 0.7847\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 1s 86ms/step - loss: 0.4502 - accuracy: 0.8155\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.5095 - accuracy: 0.7425\n",
      "Loss:  0.509497880935669\n",
      "Accuracy:  0.7425312399864197\n",
      "Reading Training Data\n",
      "716  Data Points Read!\n",
      "729  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1445  Data Points\n",
      "Epoch 1/5\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 0.5451 - accuracy: 0.7294\n",
      "Epoch 2/5\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.4875 - accuracy: 0.7862\n",
      "Epoch 3/5\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 0.4477 - accuracy: 0.8125\n",
      "Epoch 4/5\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.3795 - accuracy: 0.8436\n",
      "Epoch 5/5\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.3321 - accuracy: 0.8567\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.3390 - accuracy: 0.8666\n",
      "Loss:  0.3389522135257721\n",
      "Accuracy:  0.8665580153465271\n",
      "Reading Training Data\n",
      "530  Data Points Read!\n",
      "572  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1102  Data Points\n",
      "Epoch 1/5\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 0.5601 - accuracy: 0.7223\n",
      "Epoch 2/5\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 0.5249 - accuracy: 0.7559\n",
      "Epoch 3/5\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 0.5065 - accuracy: 0.7632\n",
      "Epoch 4/5\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 0.5124 - accuracy: 0.7459\n",
      "Epoch 5/5\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 0.4679 - accuracy: 0.7849\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.4411 - accuracy: 0.8197\n",
      "Loss:  0.4410892724990845\n",
      "Accuracy:  0.8196632266044617\n",
      "Reading Training Data\n",
      "695  Data Points Read!\n",
      "701  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1396  Data Points\n",
      "Epoch 1/5\n",
      "14/14 [==============================] - 1s 96ms/step - loss: 0.5409 - accuracy: 0.7400\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 1s 95ms/step - loss: 0.4955 - accuracy: 0.7758\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 1s 94ms/step - loss: 0.4393 - accuracy: 0.8059\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 1s 92ms/step - loss: 0.3913 - accuracy: 0.8431\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 1s 92ms/step - loss: 0.3355 - accuracy: 0.8603\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.3547 - accuracy: 0.8577\n",
      "Loss:  0.3547351360321045\n",
      "Accuracy:  0.8576860427856445\n"
     ]
    }
   ],
   "source": [
    "globalDict['global2'] = trainingDict['global2']\n",
    "trainingDict['d6'] = local_train('./input/fed/d6', 'd6', 2)\n",
    "trainingDict['d7'] = local_train('./input/fed/d7', 'd7', 2)\n",
    "trainingDict['d8'] = local_train('./input/fed/d8', 'd8', 2)\n",
    "trainingDict['d9'] = local_train('./input/fed/d9', 'd9', 2)\n",
    "trainingDict['d10'] = local_train('./input/fed/d10', 'd10', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeno Selections:  ['d7', 'd6', 'd9']\n",
      "Total number of data points after this round:  13296\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.4655 - accuracy: 0.7773\n",
      "Loss:  0.4655492901802063\n",
      "Accuracy:  0.777294933795929\n"
     ]
    }
   ],
   "source": [
    "NewGlobal, dataLen = zeno(trainingDict, 0.0005, 0.75)\n",
    "trainingDict = {}\n",
    "newModelLoss, newModelAccuracy = saveModel(NewGlobal, '3')\n",
    "trainingDict['global3'] = (dataLen, newModelLoss, newModelAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Training Data\n",
      "557  Data Points Read!\n",
      "577  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1134  Data Points\n",
      "Epoch 1/5\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.5010 - accuracy: 0.7584\n",
      "Epoch 2/5\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.4358 - accuracy: 0.8113\n",
      "Epoch 3/5\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.3958 - accuracy: 0.8236\n",
      "Epoch 4/5\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.3505 - accuracy: 0.8607\n",
      "Epoch 5/5\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.3044 - accuracy: 0.8783\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.3241 - accuracy: 0.8624\n",
      "Loss:  0.3241368532180786\n",
      "Accuracy:  0.8623936176300049\n",
      "Reading Training Data\n",
      "827  Data Points Read!\n",
      "796  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1623  Data Points\n",
      "Epoch 1/5\n",
      "17/17 [==============================] - 2s 93ms/step - loss: 0.4299 - accuracy: 0.8115\n",
      "Epoch 2/5\n",
      "17/17 [==============================] - 2s 94ms/step - loss: 0.3508 - accuracy: 0.8527\n",
      "Epoch 3/5\n",
      "17/17 [==============================] - 2s 94ms/step - loss: 0.2990 - accuracy: 0.8805\n",
      "Epoch 4/5\n",
      "17/17 [==============================] - 2s 95ms/step - loss: 0.2512 - accuracy: 0.9051\n",
      "Epoch 5/5\n",
      "17/17 [==============================] - 2s 95ms/step - loss: 0.2088 - accuracy: 0.9224\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.2458 - accuracy: 0.8993\n",
      "Loss:  0.24576102197170258\n",
      "Accuracy:  0.8993300795555115\n",
      "Reading Training Data\n",
      "395  Data Points Read!\n",
      "425  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "820  Data Points\n",
      "Epoch 1/5\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 0.4720 - accuracy: 0.7915\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 0.4475 - accuracy: 0.8061\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 1s 86ms/step - loss: 0.4172 - accuracy: 0.8207\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 1s 86ms/step - loss: 0.3820 - accuracy: 0.8476\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 1s 88ms/step - loss: 0.3429 - accuracy: 0.8634\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.3545 - accuracy: 0.8430\n",
      "Loss:  0.3544818162918091\n",
      "Accuracy:  0.8430200815200806\n",
      "Reading Training Data\n",
      "513  Data Points Read!\n",
      "528  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1041  Data Points\n",
      "Epoch 1/5\n",
      "11/11 [==============================] - 1s 90ms/step - loss: 0.4851 - accuracy: 0.7781\n",
      "Epoch 2/5\n",
      "11/11 [==============================] - 1s 90ms/step - loss: 0.4389 - accuracy: 0.7992\n",
      "Epoch 3/5\n",
      "11/11 [==============================] - 1s 91ms/step - loss: 0.4032 - accuracy: 0.8223\n",
      "Epoch 4/5\n",
      "11/11 [==============================] - 1s 92ms/step - loss: 0.3668 - accuracy: 0.8501\n",
      "Epoch 5/5\n",
      "11/11 [==============================] - 1s 90ms/step - loss: 0.3310 - accuracy: 0.8636\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.3144 - accuracy: 0.8599\n",
      "Loss:  0.3144351840019226\n",
      "Accuracy:  0.8598587512969971\n",
      "Reading Training Data\n",
      "284  Data Points Read!\n",
      "281  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "565  Data Points\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.4989 - accuracy: 0.7451\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.4389 - accuracy: 0.8407\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.4280 - accuracy: 0.8035\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.3889 - accuracy: 0.8602\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.3580 - accuracy: 0.8655\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.3880 - accuracy: 0.8418\n",
      "Loss:  0.3879523277282715\n",
      "Accuracy:  0.8417526483535767\n"
     ]
    }
   ],
   "source": [
    "globalDict['global3'] = trainingDict['global3']\n",
    "trainingDict['d11'] = local_train('./input/fed/d11', 'd11', 3)\n",
    "trainingDict['d12'] = local_train('./input/fed/d12', 'd12', 3)\n",
    "trainingDict['d13'] = local_train('./input/fed/d13', 'd13', 3)\n",
    "trainingDict['d14'] = local_train('./input/fed/d14', 'd14', 3)\n",
    "trainingDict['d15'] = local_train('./input/fed/d15', 'd15', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeno Selections:  ['d15', 'd13', 'd14']\n",
      "Total number of data points after this round:  18479\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.4200 - accuracy: 0.8050\n",
      "Loss:  0.4200378656387329\n",
      "Accuracy:  0.8049972653388977\n"
     ]
    }
   ],
   "source": [
    "NewGlobal, dataLen = zeno(trainingDict, 0.0005, 0.75)\n",
    "trainingDict = {}\n",
    "newModelLoss, newModelAccuracy = saveModel(NewGlobal, '4')\n",
    "trainingDict['global4'] = (dataLen, newModelLoss, newModelAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Training Data\n",
      "412  Data Points Read!\n",
      "416  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "828  Data Points\n",
      "Epoch 1/5\n",
      "9/9 [==============================] - 1s 79ms/step - loss: 0.4813 - accuracy: 0.7838\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 1s 80ms/step - loss: 0.4078 - accuracy: 0.8345\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 1s 79ms/step - loss: 0.3486 - accuracy: 0.8623\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 1s 79ms/step - loss: 0.2994 - accuracy: 0.8816\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 1s 78ms/step - loss: 0.2582 - accuracy: 0.9010\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.2890 - accuracy: 0.8847\n",
      "Loss:  0.28904205560684204\n",
      "Accuracy:  0.8846641182899475\n",
      "Reading Training Data\n",
      "417  Data Points Read!\n",
      "414  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "831  Data Points\n",
      "Epoch 1/5\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 0.4406 - accuracy: 0.8075\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 1s 85ms/step - loss: 0.3703 - accuracy: 0.8508\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 1s 88ms/step - loss: 0.3368 - accuracy: 0.8628\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 1s 93ms/step - loss: 0.2884 - accuracy: 0.9013\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 1s 87ms/step - loss: 0.2670 - accuracy: 0.8929\n",
      "173/173 [==============================] - 2s 11ms/step - loss: 0.3174 - accuracy: 0.8582\n",
      "Loss:  0.3174448013305664\n",
      "Accuracy:  0.8582292199134827\n",
      "Reading Training Data\n",
      "269  Data Points Read!\n",
      "252  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "521  Data Points\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.4856 - accuracy: 0.7620\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.4483 - accuracy: 0.8023\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.4039 - accuracy: 0.8464\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.3988 - accuracy: 0.8100\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.3695 - accuracy: 0.8656\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.3725 - accuracy: 0.8280\n",
      "Loss:  0.3724521994590759\n",
      "Accuracy:  0.8279920220375061\n",
      "Reading Training Data\n",
      "407  Data Points Read!\n",
      "407  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "814  Data Points\n",
      "Epoch 1/5\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 0.4605 - accuracy: 0.8182\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 1s 87ms/step - loss: 0.3932 - accuracy: 0.8649\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 1s 85ms/step - loss: 0.3469 - accuracy: 0.8538\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 1s 81ms/step - loss: 0.2958 - accuracy: 0.8919\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 0.2687 - accuracy: 0.8907\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.2911 - accuracy: 0.8749\n",
      "Loss:  0.291105180978775\n",
      "Accuracy:  0.8748868107795715\n",
      "Reading Training Data\n",
      "286  Data Points Read!\n",
      "276  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "562  Data Points\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 0.4916 - accuracy: 0.7829\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 0.4329 - accuracy: 0.8203\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 0.4031 - accuracy: 0.8559\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 0.3660 - accuracy: 0.8523\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 0.3306 - accuracy: 0.8772\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.3384 - accuracy: 0.8566\n",
      "Loss:  0.3383769094944\n",
      "Accuracy:  0.8565996885299683\n"
     ]
    }
   ],
   "source": [
    "globalDict['global4'] = trainingDict['global4']\n",
    "trainingDict['d16'] = local_train('./input/fed/d16', 'd16', 4)\n",
    "trainingDict['d17'] = local_train('./input/fed/d17', 'd17', 4)\n",
    "trainingDict['d18'] = local_train('./input/fed/d18', 'd18', 4)\n",
    "trainingDict['d19'] = local_train('./input/fed/d19', 'd19', 4)\n",
    "trainingDict['d20'] = local_train('./input/fed/d20', 'd20', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeno Selections:  ['d20', 'd18', 'd19']\n",
      "Total number of data points after this round:  22035\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.4009 - accuracy: 0.8142\n",
      "Loss:  0.400885671377182\n",
      "Accuracy:  0.8142313957214355\n"
     ]
    }
   ],
   "source": [
    "NewGlobal, dataLen = zeno(trainingDict, 0.0005, 0.75)\n",
    "trainingDict = {}\n",
    "newModelLoss, newModelAccuracy = saveModel(NewGlobal, '5')\n",
    "trainingDict['global5'] = (dataLen, newModelLoss, newModelAccuracy)\n",
    "globalDict['global5'] = trainingDict['global5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global1': (1382, 0.6062763929367065, 0.670106828212738),\n",
       " 'global2': (7374, 0.5276505947113037, 0.7358319759368896),\n",
       " 'global3': (13296, 0.4655492901802063, 0.777294933795929),\n",
       " 'global4': (18479, 0.4200378656387329, 0.8049972653388977),\n",
       " 'global5': (22035, 0.400885671377182, 0.8142313957214355)}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalDict\n",
    "\n",
    "#Zeno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
